#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Genesis Gradio çœŸå®ç”Ÿæˆç•Œé¢
ä½¿ç”¨ diffusers åº“å®ç°çœŸå®çš„å›¾åƒç”Ÿæˆ

ä¾èµ–:
    pip install gradio diffusers transformers accelerate
"""

import sys
from pathlib import Path
import time
import torch

sys.path.insert(0, str(Path(__file__).parent))

try:
    import gradio as gr
    print(f"Gradio ç‰ˆæœ¬: {gr.__version__}")
except ImportError:
    print("è¯·å®‰è£… Gradio: pip install gradio")
    sys.exit(1)

try:
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
    print("âœ“ diffusers å·²å®‰è£…")
except ImportError:
    print("âŒ è¯·å®‰è£… diffusers: pip install diffusers transformers accelerate")
    sys.exit(1)

# å¯¼å…¥ folder_paths
import importlib.util
spec = importlib.util.spec_from_file_location(
    "folder_paths", 
    Path(__file__).parent / "core" / "folder_paths.py"
)
folder_paths = importlib.util.module_from_spec(spec)
spec.loader.exec_module(folder_paths)


class GenesisGenerator:
    """Genesis å›¾åƒç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        self.current_model = None
        
    def load_model(self, model_name, progress=None):
        """åŠ è½½æ¨¡å‹"""
        try:
            if progress:
                progress(0.1, desc="æ­£åœ¨åŠ è½½æ¨¡å‹...")
            
            # åˆ¤æ–­æ˜¯æœ¬åœ°æ¨¡å‹è¿˜æ˜¯ HuggingFace æ¨¡å‹
            if model_name.startswith("HF:"):
                # HuggingFace æ¨¡å‹
                model_id = model_name[3:]  # å»æ‰ "HF:" å‰ç¼€
                print(f"åŠ è½½ HuggingFace æ¨¡å‹: {model_id}")
                model_path = model_id
            else:
                # æœ¬åœ°æ¨¡å‹
                print(f"åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
                model_path = folder_paths.get_full_path('checkpoints', model_name)
                print(f"æ¨¡å‹è·¯å¾„: {model_path}")
            
            print(f"è®¾å¤‡: {self.device}")
            
            if progress:
                progress(0.3, desc="åŠ è½½æ¨¡å‹æ–‡ä»¶...")
            
            # åŠ è½½ Stable Diffusion pipeline
            try:
                from diffusers import StableDiffusionPipeline
                self.pipe = StableDiffusionPipeline.from_single_file(
                    model_path,
                    torch_dtype=self.dtype,
                    safety_checker=None,
                    requires_safety_checker=False,
                    load_safety_checker=False
                )
            except Exception as e:
                # å¦‚æœ from_single_file å¤±è´¥ï¼Œå°è¯• from_pretrained
                print(f"å°è¯•ä½¿ç”¨ from_pretrained åŠ è½½...")
                self.pipe = StableDiffusionPipeline.from_pretrained(
                    model_path,
                    torch_dtype=self.dtype,
                    safety_checker=None,
                    requires_safety_checker=False
                )
            
            if progress:
                progress(0.6, desc="å°†æ¨¡å‹ç§»è‡³è®¾å¤‡...")
            
            self.pipe = self.pipe.to(self.device)
            
            if progress:
                progress(0.8, desc="é…ç½®è°ƒåº¦å™¨...")
            
            # ä½¿ç”¨ Euler è°ƒåº¦å™¨
            self.pipe.scheduler = EulerDiscreteScheduler.from_config(
                self.pipe.scheduler.config
            )
            
            # å¯ç”¨ä¼˜åŒ–
            if self.device == "cuda":
                # å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ä»¥èŠ‚çœæ˜¾å­˜
                self.pipe.enable_attention_slicing()
                
                # å¦‚æœæœ‰ xformersï¼Œå¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                    print("âœ“ å¯ç”¨ xformers ä¼˜åŒ–")
                except:
                    print("â„¹ xformers æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›")
            
            self.current_model = model_name
            
            if progress:
                progress(1.0, desc="æ¨¡å‹åŠ è½½å®Œæˆ!")
            
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
            return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}"
            
        except Exception as e:
            import traceback
            error_msg = f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}\n\n{traceback.format_exc()}"
            print(error_msg)
            return error_msg
    
    def generate(
        self,
        prompt,
        negative_prompt="",
        width=512,
        height=512,
        steps=20,
        cfg_scale=7.0,
        seed=-1,
        progress=gr.Progress()
    ):
        """ç”Ÿæˆå›¾åƒ"""
        if self.pipe is None:
            return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        try:
            # è®¾ç½®éšæœºç§å­
            if seed == -1:
                seed = torch.randint(0, 2**32 - 1, (1,)).item()
            
            generator = torch.Generator(device=self.device).manual_seed(int(seed))
            
            # ç”Ÿæˆå›¾åƒ
            progress(0, desc="å¼€å§‹ç”Ÿæˆ...")
            
            def callback(step, timestep, latents):
                progress(step / steps, desc=f"ç”Ÿæˆä¸­... {step}/{steps}")
            
            result = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=cfg_scale,
                generator=generator,
                callback=callback,
                callback_steps=1
            )
            
            image = result.images[0]
            
            progress(1.0, desc="å®Œæˆ!")
            
            info = f"""
## âœ… ç”ŸæˆæˆåŠŸï¼

**æç¤ºè¯:** {prompt}

**å‚æ•°:**
- å°ºå¯¸: {width} x {height}
- æ­¥æ•°: {steps}
- CFG: {cfg_scale}
- ç§å­: {seed}
- è®¾å¤‡: {self.device}
"""
            
            return image, info
            
        except Exception as e:
            import traceback
            error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
            print(error_msg)
            return None, error_msg


def get_models():
    """è·å–æ¨¡å‹åˆ—è¡¨"""
    return {
        'checkpoints': folder_paths.get_filename_list('checkpoints'),
        'loras': folder_paths.get_filename_list('loras'),
        'vaes': folder_paths.get_filename_list('vae')
    }


def show_models():
    """æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯"""
    models = get_models()
    
    info = f"""
## ğŸ“¦ å¯ç”¨æ¨¡å‹ç»Ÿè®¡

- **Checkpoints:** {len(models['checkpoints'])} ä¸ª
- **LoRAs:** {len(models['loras'])} ä¸ª
- **VAEs:** {len(models['vaes'])} ä¸ª

---

### Checkpoints
"""
    
    if models['checkpoints']:
        for i, name in enumerate(models['checkpoints'][:10], 1):
            info += f"{i}. `{name}`\n"
        if len(models['checkpoints']) > 10:
            info += f"\n... è¿˜æœ‰ {len(models['checkpoints']) - 10} ä¸ª\n"
    else:
        info += "*æœªæ‰¾åˆ°æ¨¡å‹*\n"
    
    return info


# åˆ›å»ºå…¨å±€ç”Ÿæˆå™¨
generator = GenesisGenerator()


def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # è·å–å¯ç”¨æ¨¡å‹
    models = get_models()
    
    # æ„å»ºæ¨¡å‹é€‰æ‹©åˆ—è¡¨
    checkpoint_choices = []
    
    # æ·»åŠ  HuggingFace é»˜è®¤æ¨¡å‹
    checkpoint_choices.append("HF:runwayml/stable-diffusion-v1-5")
    checkpoint_choices.append("HF:stabilityai/stable-diffusion-2-1")
    
    # æ·»åŠ æœ¬åœ°æ¨¡å‹
    if models['checkpoints']:
        checkpoint_choices.extend(models['checkpoints'])
    
    with gr.Blocks(
        title="Genesis AI - çœŸå®ç”Ÿæˆ",
        theme=gr.themes.Soft(primary_hue="purple")
    ) as demo:
        
        gr.Markdown("""
        # ğŸ¨ Genesis AI å›¾åƒç”Ÿæˆå™¨
        
        **çœŸå®å›¾åƒç”Ÿæˆ** - åŸºäº Stable Diffusion
        """)
        
        with gr.Tabs():
            # ç”Ÿæˆæ ‡ç­¾é¡µ
            with gr.Tab("ğŸ¨ å›¾åƒç”Ÿæˆ"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¯ æ¨¡å‹é€‰æ‹©")
                        
                        model_selector = gr.Dropdown(
                            label="é€‰æ‹©æ¨¡å‹",
                            choices=checkpoint_choices,
                            value=checkpoint_choices[0] if checkpoint_choices else None,
                            interactive=True
                        )
                        
                        load_model_btn = gr.Button(
                            "ğŸ“¥ åŠ è½½æ¨¡å‹",
                            variant="secondary"
                        )
                        
                        model_status = gr.Textbox(
                            label="æ¨¡å‹çŠ¶æ€",
                            value="æœªåŠ è½½æ¨¡å‹",
                            interactive=False,
                            lines=2
                        )
                        
                        gr.Markdown("### ğŸ“ ç”Ÿæˆè®¾ç½®")
                        
                        prompt = gr.Textbox(
                            label="æ­£å‘æç¤ºè¯",
                            placeholder="æè¿°ä½ æƒ³ç”Ÿæˆçš„å›¾åƒ...",
                            lines=3,
                            value="a beautiful landscape with mountains and lake, sunset, 4k, highly detailed"
                        )
                        
                        negative_prompt = gr.Textbox(
                            label="è´Ÿå‘æç¤ºè¯",
                            placeholder="è¦é¿å…çš„å†…å®¹...",
                            lines=2,
                            value="ugly, blurry, low quality, distorted, bad anatomy"
                        )
                        
                        gr.Markdown("### âš™ï¸ å‚æ•°è®¾ç½®")
                        
                        with gr.Row():
                            width = gr.Slider(
                                label="å®½åº¦",
                                minimum=256,
                                maximum=1024,
                                step=64,
                                value=512
                            )
                            
                            height = gr.Slider(
                                label="é«˜åº¦",
                                minimum=256,
                                maximum=1024,
                                step=64,
                                value=512
                            )
                        
                        with gr.Row():
                            steps = gr.Slider(
                                label="é‡‡æ ·æ­¥æ•°",
                                minimum=1,
                                maximum=100,
                                step=1,
                                value=20
                            )
                            
                            cfg_scale = gr.Slider(
                                label="CFG Scale",
                                minimum=1.0,
                                maximum=20.0,
                                step=0.5,
                                value=7.0
                            )
                        
                        seed = gr.Number(
                            label="ç§å­ (-1 ä¸ºéšæœº)",
                            value=-1,
                            precision=0
                        )
                        
                        generate_btn = gr.Button(
                            "ğŸ¨ ç”Ÿæˆå›¾åƒ",
                            variant="primary",
                            size="lg"
                        )
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ–¼ï¸ ç”Ÿæˆç»“æœ")
                        
                        output_image = gr.Image(
                            label="ç”Ÿæˆçš„å›¾åƒ",
                            type="pil"
                        )
                        
                        output_info = gr.Markdown(
                            value="ç‚¹å‡»ã€Œç”Ÿæˆå›¾åƒã€æŒ‰é’®å¼€å§‹..."
                        )
                
                # ç¤ºä¾‹
                gr.Markdown("### ğŸ’¡ é¢„è®¾ç¤ºä¾‹")
                
                gr.Examples(
                    examples=[
                        [
                            "a serene mountain landscape at sunset, beautiful colors, 4k",
                            "ugly, blurry, low quality",
                            512, 512, 20, 7.0, -1
                        ],
                        [
                            "a cute cat sitting on a windowsill, soft lighting, detailed fur",
                            "distorted, ugly, bad anatomy",
                            512, 512, 25, 7.5, 42
                        ],
                        [
                            "cyberpunk city at night, neon lights, futuristic, highly detailed",
                            "blurry, low quality, bad composition",
                            768, 512, 30, 8.0, 123
                        ],
                    ],
                    inputs=[prompt, negative_prompt, width, height, steps, cfg_scale, seed]
                )
                
                # è¿æ¥åŠ è½½æ¨¡å‹æŒ‰é’®
                load_model_btn.click(
                    fn=generator.load_model,
                    inputs=[model_selector],
                    outputs=[model_status]
                )
                
                # è¿æ¥ç”ŸæˆæŒ‰é’®
                generate_btn.click(
                    fn=generator.generate,
                    inputs=[
                        prompt, negative_prompt,
                        width, height,
                        steps, cfg_scale, seed
                    ],
                    outputs=[output_image, output_info]
                )
            
            # æ¨¡å‹ç®¡ç†æ ‡ç­¾é¡µ
            with gr.Tab("ğŸ“¦ æ¨¡å‹ç®¡ç†"):
                gr.Markdown(show_models())
                
                gr.Markdown(f"""
                ### ğŸ”§ æ¨¡å‹é€‰æ‹©è¯´æ˜
                
                **æœ¬åœ°æ¨¡å‹:**
                - ä½ æœ‰ {len(models['checkpoints'])} ä¸ªæœ¬åœ° checkpoint æ¨¡å‹
                - è¿™äº›æ¨¡å‹æ¥è‡ªä½ é…ç½®çš„è·¯å¾„ï¼ˆextra_model_paths.yamlï¼‰
                - ç›´æ¥é€‰æ‹©æ¨¡å‹åç§°å³å¯åŠ è½½
                
                **HuggingFace æ¨¡å‹:**
                - ä»¥ `HF:` å¼€å¤´çš„æ˜¯åœ¨çº¿æ¨¡å‹
                - `HF:runwayml/stable-diffusion-v1-5` - SD 1.5ï¼ˆæ¨èï¼‰
                - `HF:stabilityai/stable-diffusion-2-1` - SD 2.1
                - é¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆçº¦ 4GBï¼‰
                - æ¨¡å‹ä¼šç¼“å­˜åˆ°: `~/.cache/huggingface/`
                
                ### ğŸ’¡ ä½¿ç”¨æ­¥éª¤
                
                1. åœ¨ã€Œå›¾åƒç”Ÿæˆã€æ ‡ç­¾é¡µé€‰æ‹©æ¨¡å‹
                2. ç‚¹å‡»ã€ŒğŸ“¥ åŠ è½½æ¨¡å‹ã€æŒ‰é’®
                3. ç­‰å¾…åŠ è½½å®Œæˆ
                4. å¼€å§‹ç”Ÿæˆå›¾åƒ
                """)
            
            # å…³äºæ ‡ç­¾é¡µ
            with gr.Tab("â„¹ï¸ å…³äº"):
                gr.Markdown(f"""
                # Genesis AI Engine
                
                ## ğŸš€ å½“å‰çŠ¶æ€
                
                - **è®¾å¤‡:** {generator.device}
                - **ç²¾åº¦:** {generator.dtype}
                - **CUDA å¯ç”¨:** {'æ˜¯' if torch.cuda.is_available() else 'å¦'}
                
                ## ğŸ“¦ ä¾èµ–
                
                - **Gradio:** {gr.__version__}
                - **PyTorch:** {torch.__version__}
                - **Diffusers:** å·²å®‰è£…
                
                ## ğŸ’¡ ä½¿ç”¨è¯´æ˜
                
                1. é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ SD 1.5 æ¨¡å‹ï¼ˆçº¦4GBï¼‰
                2. è¾“å…¥æç¤ºè¯æè¿°æƒ³è¦ç”Ÿæˆçš„å›¾åƒ
                3. è°ƒæ•´å‚æ•°ï¼ˆå°ºå¯¸ã€æ­¥æ•°ã€CFGç­‰ï¼‰
                4. ç‚¹å‡»ç”ŸæˆæŒ‰é’®
                5. ç­‰å¾…ç”Ÿæˆå®Œæˆ
                
                ## âš¡ æ€§èƒ½ä¼˜åŒ–
                
                - ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
                - å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡èŠ‚çœæ˜¾å­˜
                - æ”¯æŒ xformers ä¼˜åŒ–ï¼ˆå¦‚å·²å®‰è£…ï¼‰
                
                ## ğŸ‘¨â€ğŸ’» ä½œè€…
                
                **eddy** - 2025-11-13
                """)
    
    return demo


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("Genesis AI - çœŸå®å›¾åƒç”Ÿæˆç•Œé¢")
    print("=" * 70)
    print()
    
    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {generator.device}")
    print(f"ğŸ“Š ç²¾åº¦: {generator.dtype}")
    
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ’¾ æ˜¾å­˜: {vram:.1f} GB")
    
    print()
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    models = get_models()
    print(f"ğŸ“¦ å¯ç”¨æ¨¡å‹:")
    print(f"  - Checkpoints: {len(models['checkpoints'])} ä¸ª")
    print(f"  - LoRAs: {len(models['loras'])} ä¸ª")
    print(f"  - VAEs: {len(models['vaes'])} ä¸ª")
    print()
    
    print("ğŸš€ å¯åŠ¨ Gradio ç•Œé¢...")
    print("=" * 70)
    print()
    
    print("ğŸ’¡ æç¤º:")
    print("   - åœ¨ç•Œé¢ä¸­é€‰æ‹©æ¨¡å‹åï¼Œç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€æŒ‰é’®")
    print("   - æ”¯æŒæœ¬åœ° checkpoint æ¨¡å‹å’Œ HuggingFace æ¨¡å‹")
    print("   - HuggingFace æ¨¡å‹é¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½")
    print()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_ui()
    
    # å°è¯•å¤šç§å¯åŠ¨æ–¹å¼
    try:
        print("å°è¯•å¯åŠ¨æ–¹å¼ 1: é»˜è®¤ç«¯å£ 7860...")
        demo.launch(
            server_port=7860,
            share=False,
            inbrowser=True
        )
    except Exception as e:
        print(f"æ–¹å¼ 1 å¤±è´¥: {e}")
        print("\nå°è¯•å¯åŠ¨æ–¹å¼ 2: è‡ªåŠ¨é€‰æ‹©ç«¯å£...")
        try:
            demo.launch(
                server_port=0,  # è‡ªåŠ¨é€‰æ‹©å¯ç”¨ç«¯å£
                share=False,
                inbrowser=True
            )
        except Exception as e2:
            print(f"æ–¹å¼ 2 å¤±è´¥: {e2}")
            print("\nå°è¯•å¯åŠ¨æ–¹å¼ 3: ä½¿ç”¨å…¬å…±é“¾æ¥...")
            try:
                demo.queue()  # å¯ç”¨é˜Ÿåˆ—
                demo.launch(
                    share=True,
                    inbrowser=True
                )
            except Exception as e3:
                print(f"æ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥äº†: {e3}")
                print("\nè¯·å°è¯•:")
                print("1. å…³é—­å…¶ä»–å ç”¨ç«¯å£çš„ç¨‹åº")
                print("2. é‡å¯ç”µè„‘")
                print("3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
