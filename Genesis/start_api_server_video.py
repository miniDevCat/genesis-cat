#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Genesis API Server - æ”¯æŒæ–‡ç”Ÿå›¾å’Œæ–‡ç”Ÿè§†é¢‘
ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
"""

import sys
import os
from pathlib import Path
import base64
from io import BytesIO

# ç¦ç”¨æ‰€æœ‰å¯é€‰çš„æ³¨æ„åŠ›ä¼˜åŒ–åº“ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
os.environ['DIFFUSERS_DISABLE_FLASH_ATTENTION'] = '1'
os.environ['DISABLE_SAGE_ATTENTION'] = '1'
os.environ['ATTN_BACKEND'] = 'pytorch'  # å¼ºåˆ¶ä½¿ç”¨ PyTorch åŸç”Ÿå®ç°

# è®¾ç½® ComfyUI è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
os.environ['COMFYUI_PATH'] = project_root
sys.path.insert(0, project_root)

try:
    from flask import Flask, request, jsonify, send_file
    from flask_cors import CORS
    FLASK_AVAILABLE = True
except ImportError:
    print("=" * 70)
    print("é”™è¯¯: Flask æœªå®‰è£…")
    print("=" * 70)
    print()
    print("è¯·å®‰è£…å¿…è¦çš„ä¾èµ–:")
    print("  pip install flask flask-cors")
    print()
    sys.exit(1)

try:
    import torch
    import numpy as np
    from PIL import Image
    TORCH_AVAILABLE = True
    print("âœ“ PyTorch å·²å®‰è£…")
except ImportError:
    print("âš ï¸  PyTorch æœªå®‰è£…")
    TORCH_AVAILABLE = False

import uuid
import time
import threading
from queue import Queue, Empty
from datetime import datetime
import json

# åˆ›å»º Flask åº”ç”¨
app = Flask(__name__)
CORS(app)

# ä»»åŠ¡å­˜å‚¨
tasks = {}
task_queue = Queue()

# ä»»åŠ¡çŠ¶æ€å¸¸é‡
class TaskStatus:
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

# ä»»åŠ¡ç±»
class Task:
    def __init__(self, task_id, task_type, params):
        self.task_id = task_id
        self.task_type = task_type
        self.params = params
        self.status = TaskStatus.PENDING
        self.progress = 0
        self.result = None
        self.error = None
        self.created_at = datetime.now()
        self.started_at = None
        self.completed_at = None
    
    def to_dict(self):
        return {
            'task_id': self.task_id,
            'task_type': self.task_type,
            'params': self.params,
            'status': self.status,
            'progress': self.progress,
            'result': self.result,
            'error': self.error,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'started_at': self.started_at.isoformat() if self.started_at else None,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None
        }

# æ¨¡å‹è·¯å¾„é…ç½®
IMAGE_MODELS = {
    'sd-v1-5': {
        'name': 'Stable Diffusion v1.5',
        'path': 'runwayml/stable-diffusion-v1-5',
        'type': 'huggingface'
    },
    'sd-v2-1': {
        'name': 'Stable Diffusion v2.1',
        'path': 'stabilityai/stable-diffusion-2-1',
        'type': 'huggingface'
    },
}

VIDEO_MODELS = {
    'wan2.2-i2v': {
        'name': 'Wan2.2 I2V (NSFW)',
        'path': r'E:\fuxkcomfy_windows_portable\FuxkComfy\models\diffusion_models\wan2.2-i2v-rapid-aio-nsfw-v9.2.safetensors',
        'type': 'i2v',
        'description': 'å›¾ç”Ÿè§†é¢‘æ¨¡å‹ï¼Œå¿«é€Ÿç”Ÿæˆ'
    },
    'wan2-icecannon-t2v': {
        'name': 'Wan2 IceCannon T2V (NSFW)',
        'path': r'E:\fuxkcomfy_windows_portable\FuxkComfy\models\diffusion_models\Wan2_IceCannon_t2v2.1_nsfw_RCM_Lab_4step.safetensors',
        'type': 't2v',
        'description': 'æ–‡ç”Ÿè§†é¢‘æ¨¡å‹ï¼Œ4æ­¥å¿«é€Ÿç”Ÿæˆ'
    },
    'svd-img2vid': {
        'name': 'Stable Video Diffusion',
        'path': 'stabilityai/stable-video-diffusion-img2vid',
        'type': 'i2v',
        'description': 'å®˜æ–¹å›¾ç”Ÿè§†é¢‘æ¨¡å‹'
    },
}

# é»˜è®¤æ¨¡å‹
DEFAULT_IMAGE_MODEL = 'sd-v1-5'
DEFAULT_VIDEO_MODEL = 'wan2.2-i2v'

# å…¨å±€ç”Ÿæˆå™¨
class MultiModalGenerator:
    def __init__(self):
        self.image_pipe = None
        self.video_pipe = None
        self.wan_video_workflow = None
        self.device = "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32 if TORCH_AVAILABLE else None
        self.image_model_loaded = False
        self.video_model_loaded = False
        self.current_image_model = None
        self.current_video_model = None
        
        print("\n" + "="*60)
        print("åˆå§‹åŒ–å¤šæ¨¡æ€ç”Ÿæˆå™¨")
        print("="*60)
        
        # å°è¯•åŠ è½½ WanVideo å·¥ä½œæµ
        try:
            self._init_wanvideo_workflow()
        except Exception as e:
            print(f"âŒ WanVideo å·¥ä½œæµåˆå§‹åŒ–å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            self.wan_video_workflow = None
        
    def load_image_model(self, model_id=None):
        """åŠ è½½æ–‡ç”Ÿå›¾æ¨¡å‹"""
        if not TORCH_AVAILABLE:
            print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
            return False
        
        if model_id is None:
            model_id = DEFAULT_IMAGE_MODEL
        
        # å¦‚æœå·²åŠ è½½ç›¸åŒæ¨¡å‹ï¼Œè·³è¿‡
        if self.image_model_loaded and self.current_image_model == model_id:
            print(f"âœ“ å›¾åƒæ¨¡å‹å·²åŠ è½½: {model_id}")
            return True
            
        try:
            from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
            
            if model_id not in IMAGE_MODELS:
                raise ValueError(f"æœªçŸ¥çš„å›¾åƒæ¨¡å‹: {model_id}")
            
            model_info = IMAGE_MODELS[model_id]
            model_path = model_info['path']
            
            print(f"ğŸ“¥ åŠ è½½å›¾åƒæ¨¡å‹: {model_info['name']}")
            print(f"   è·¯å¾„: {model_path}")
            print(f"   è®¾å¤‡: {self.device}")
            
            self.image_pipe = StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=self.dtype,
                safety_checker=None,
                requires_safety_checker=False
            )
            
            self.image_pipe = self.image_pipe.to(self.device)
            self.image_pipe.scheduler = EulerDiscreteScheduler.from_config(
                self.image_pipe.scheduler.config
            )
            
            if self.device == "cuda":
                self.image_pipe.enable_attention_slicing()
            
            self.image_model_loaded = True
            self.current_image_model = model_id
            print(f"âœ… å›¾åƒæ¨¡å‹åŠ è½½æˆåŠŸ: {model_info['name']}")
            return True
            
        except Exception as e:
            print(f"âŒ å›¾åƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _init_wanvideo_workflow(self):
        """åˆå§‹åŒ– WanVideo å·¥ä½œæµ"""
        try:
            # è®¾ç½®æ­£ç¡®çš„è·¯å¾„
            genesis_root = os.path.dirname(current_dir)  # E:\Comfyu3.13---test
            genesis_main = current_dir  # E:\Comfyu3.13---test\Genesis-main
            apps_path = os.path.join(genesis_main, 'apps')
            
            print(f"ğŸ” å°è¯•åŠ è½½ WanVideo å·¥ä½œæµ...")
            print(f"   Genesis æ ¹ç›®å½•: {genesis_root}")
            print(f"   Genesis-main: {genesis_main}")
            print(f"   Apps è·¯å¾„: {apps_path}")
            
            if not os.path.exists(apps_path):
                print(f"   âŒ Apps ç›®å½•ä¸å­˜åœ¨")
                self.wan_video_workflow = None
                return False
            
            # æ·»åŠ å¿…è¦çš„è·¯å¾„åˆ° sys.path
            if genesis_root not in sys.path:
                sys.path.insert(0, genesis_root)
            if genesis_main not in sys.path:
                sys.path.insert(0, genesis_main)
            if apps_path not in sys.path:
                sys.path.insert(0, apps_path)
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['COMFYUI_PATH'] = genesis_root
            
            # åˆ›å»º genesis æ¨¡å—åˆ«åï¼ŒæŒ‡å‘ Genesis-main
            print("   â³ åˆ›å»ºæ¨¡å—åˆ«å...")
            import importlib.util
            
            # å°† Genesis-main ä½œä¸º genesis æ¨¡å—å¯¼å…¥
            spec = importlib.util.spec_from_file_location("genesis", os.path.join(genesis_main, "__init__.py"))
            if spec and spec.loader:
                genesis_module = importlib.util.module_from_spec(spec)
                sys.modules['genesis'] = genesis_module
                spec.loader.exec_module(genesis_module)
                print("   âœ“ åˆ›å»º genesis æ¨¡å—åˆ«åæˆåŠŸ")
            
            print("   â³ å¯¼å…¥ WanVideo å·¥ä½œæµ...")
            from wanvideo_gradio_app import WanVideoWorkflow
            
            print("   â³ åˆå§‹åŒ– WanVideo å·¥ä½œæµ...")
            try:
                self.wan_video_workflow = WanVideoWorkflow()
                print("âœ… WanVideo å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ")
                self.video_model_loaded = True
                return True
            except RuntimeError as e:
                print(f"   âŒ WanVideo å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
                print("   è¿™æ˜¯æ­£å¸¸çš„ï¼Œè¯´æ˜ç¼ºå°‘ ComfyUI-WanVideoWrapper èŠ‚ç‚¹")
                print("   å°†ä½¿ç”¨å¤‡ç”¨çš„å›¾åƒåºåˆ—ç”Ÿæˆæ¨¡å¼")
                self.wan_video_workflow = None
                return False
        except Exception as e:
            print(f"âš ï¸  WanVideo å·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨å¤‡ç”¨æ¨¡å¼")
            import traceback
            traceback.print_exc()
            self.wan_video_workflow = None
            return False
    
    def load_video_model(self, model_id=None):
        """åŠ è½½æ–‡ç”Ÿè§†é¢‘æ¨¡å‹"""
        if not TORCH_AVAILABLE:
            print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
            return False
        
        if model_id is None:
            model_id = DEFAULT_VIDEO_MODEL
        
        # å¦‚æœå·²åŠ è½½ç›¸åŒæ¨¡å‹ï¼Œè·³è¿‡
        if self.video_model_loaded and self.current_video_model == model_id:
            print(f"âœ“ è§†é¢‘æ¨¡å‹å·²åŠ è½½: {model_id}")
            return True
            
        try:
            if model_id not in VIDEO_MODELS:
                raise ValueError(f"æœªçŸ¥çš„è§†é¢‘æ¨¡å‹: {model_id}")
            
            model_info = VIDEO_MODELS[model_id]
            model_path = model_info['path']
            
            print(f"ğŸ“¥ åŠ è½½è§†é¢‘æ¨¡å‹: {model_info['name']}")
            print(f"   è·¯å¾„: {model_path}")
            print(f"   ç±»å‹: {model_info['type']}")
            print(f"   è®¾å¤‡: {self.device}")
            
            # å°è¯•ä½¿ç”¨ diffusers åŠ è½½è§†é¢‘æ¨¡å‹
            try:
                from diffusers import StableVideoDiffusionPipeline
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ° safetensors æ–‡ä»¶
                if os.path.exists(model_path) and model_path.endswith('.safetensors'):
                    print("   å°è¯•ä» safetensors æ–‡ä»¶åŠ è½½...")
                    # ä½¿ç”¨ from_single_file åŠ è½½
                    self.video_pipe = StableVideoDiffusionPipeline.from_single_file(
                        model_path,
                        torch_dtype=self.dtype
                    )
                else:
                    # HuggingFace æ¨¡å‹
                    print("   ä» HuggingFace åŠ è½½...")
                    self.video_pipe = StableVideoDiffusionPipeline.from_pretrained(
                        model_path,
                        torch_dtype=self.dtype
                    )
                
                self.video_pipe = self.video_pipe.to(self.device)
                
                if self.device == "cuda":
                    self.video_pipe.enable_attention_slicing()
                
                print(f"âœ… è§†é¢‘æ¨¡å‹åŠ è½½æˆåŠŸ: {model_info['name']}")
                self.current_video_model = model_id
                self.video_model_loaded = True
                return True
                
            except Exception as e:
                print(f"âš ï¸  ä½¿ç”¨ diffusers åŠ è½½å¤±è´¥: {e}")
                print("   å°†ä½¿ç”¨å›¾åƒåºåˆ—æ¨¡å¼")
                self.current_video_model = model_id
                self.video_model_loaded = True  # æ ‡è®°ä¸ºå·²åŠ è½½ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                return True
            
        except Exception as e:
            print(f"âŒ è§†é¢‘æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_image(self, params, progress_callback=None):
        """ç”Ÿæˆå›¾åƒ"""
        if not TORCH_AVAILABLE:
            return self._generate_mock_image(params)
        
        # è·å–æ¨¡å‹ID
        model_id = params.get('model_id', DEFAULT_IMAGE_MODEL)
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not self.image_model_loaded or self.current_image_model != model_id:
            if not self.load_image_model(model_id):
                raise Exception("å›¾åƒæ¨¡å‹åŠ è½½å¤±è´¥")
        
        try:
            prompt = params.get('prompt', '')
            negative_prompt = params.get('negative_prompt', '')
            width = params.get('width', 512)
            height = params.get('height', 512)
            steps = params.get('steps', 20)
            cfg_scale = params.get('cfg_scale', 7.0)
            seed = params.get('seed')
            
            if seed is None or seed == -1:
                seed = torch.randint(0, 2**32 - 1, (1,)).item()
            
            generator = torch.Generator(device=self.device).manual_seed(int(seed))
            
            print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
            print(f"   æç¤ºè¯: {prompt[:50]}...")
            
            def callback(step, timestep, latents):
                if progress_callback:
                    progress = int((step / steps) * 80) + 10
                    progress_callback(progress)
            
            result = self.image_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=cfg_scale,
                generator=generator,
                callback=callback,
                callback_steps=1
            )
            
            image = result.images[0]
            
            # è½¬æ¢ä¸º base64
            buffered = BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            img_data_url = f"data:image/png;base64,{img_str}"
            
            print("âœ… å›¾åƒç”ŸæˆæˆåŠŸ")
            
            return {
                'success': True,
                'image': img_data_url,
                'seed': seed,
                'prompt': prompt,
                'width': width,
                'height': height,
            }
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def generate_video(self, params, progress_callback=None):
        """ç”Ÿæˆè§†é¢‘"""
        # è·å–æ¨¡å‹ID
        model_id = params.get('model_id', DEFAULT_VIDEO_MODEL)
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not self.video_model_loaded or self.current_video_model != model_id:
            self.load_video_model(model_id)
        
        model_info = VIDEO_MODELS.get(model_id, VIDEO_MODELS[DEFAULT_VIDEO_MODEL])
        
        # ä½¿ç”¨å¤šå¸§å›¾åƒåºåˆ—ç”ŸæˆçœŸå®è§†é¢‘
        if self.video_model_loaded:
            try:
                return self._generate_real_video(params, model_info, progress_callback)
            except Exception as e:
                print(f"âŒ çœŸå®è§†é¢‘ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼: {e}")
                import traceback
                traceback.print_exc()
                return self._generate_mock_video(params, model_info)
        else:
            print("âš ï¸  è§†é¢‘æ¨¡å‹æœªåŠ è½½")
            print(f"   å½“å‰æ¨¡å‹: {model_info['name']}")
            return self._generate_mock_video(params, model_info)
    
    def _generate_mock_image(self, params):
        """ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒ"""
        import random
        time.sleep(2)
        
        mock_image = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=="
        
        return {
            'success': True,
            'image': mock_image,
            'seed': params.get('seed') or random.randint(0, 2147483647),
            'prompt': params.get('prompt', ''),
            'width': params.get('width', 512),
            'height': params.get('height', 512),
            'note': 'âš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œè¯·å®‰è£… PyTorch å’Œ Diffusers'
        }
    
    def _generate_real_video(self, params, model_info, progress_callback=None):
        """ä½¿ç”¨çœŸå®è§†é¢‘æ¨¡å‹ç”Ÿæˆè§†é¢‘"""
        import random
        import numpy as np
        
        print("ğŸ¬ å¼€å§‹è§†é¢‘ç”Ÿæˆ...")
        print(f"   æ¨¡å‹: {model_info['name']}")
        
        # æå–å‚æ•°
        prompt = params.get('prompt', '')
        negative_prompt = params.get('negative_prompt', '')
        width = params.get('width', 512)
        height = params.get('height', 512)
        frames = params.get('frames', 16)
        steps = params.get('steps', 20)
        cfg_scale = params.get('cfg_scale', 7.5)
        seed = params.get('seed', -1)
        fps = params.get('fps', 8)
        
        if seed == -1:
            seed = random.randint(0, 2**31 - 1)
        
        # å¦‚æœæœ‰ WanVideo å·¥ä½œæµï¼Œä½¿ç”¨å®ƒ
        if self.wan_video_workflow:
            print("   ä½¿ç”¨ WanVideo å·¥ä½œæµç”Ÿæˆ...")
            return self._generate_with_wanvideo(params, model_info, seed, progress_callback)
        
        # å¦‚æœæœ‰è§†é¢‘æ¨¡å‹ï¼Œä½¿ç”¨è§†é¢‘æ¨¡å‹
        if self.video_pipe:
            print("   ä½¿ç”¨è§†é¢‘æ¨¡å‹ç›´æ¥ç”Ÿæˆ...")
            return self._generate_with_video_model(params, model_info, seed, progress_callback)
        
        # å¦åˆ™ä½¿ç”¨å›¾åƒåºåˆ—æ¨¡å¼
        print("   ä½¿ç”¨å›¾åƒåºåˆ—æ¨¡å¼...")
        
        # åŠ è½½å›¾åƒæ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆå¸§ï¼‰
        if not self.image_model_loaded:
            self.load_image_model()
        
        if not self.image_pipe:
            raise Exception("å›¾åƒæ¨¡å‹æœªåŠ è½½")
        
        print(f"   ç”Ÿæˆ {frames} å¸§å›¾åƒ...")
        print(f"   ç­–ç•¥: ç”ŸæˆåŸºç¡€å›¾åƒï¼Œç„¶åé€šè¿‡å›¾åƒå˜æ¢åˆ›å»ºå¸§åºåˆ—")
        
        # ç”Ÿæˆå¤šå¸§å›¾åƒ
        video_frames = []
        
        # å…ˆç”Ÿæˆä¸€å¼ åŸºç¡€å›¾åƒ
        print(f"   [1/2] ç”ŸæˆåŸºç¡€å›¾åƒ...")
        generator = torch.Generator(device=self.device).manual_seed(seed)
        
        with torch.no_grad():
            base_image = self.image_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=cfg_scale,
                generator=generator
            ).images[0]
        
        base_array = np.array(base_image)
        print(f"   âœ“ åŸºç¡€å›¾åƒç”Ÿæˆå®Œæˆ")
        
        # ç”Ÿæˆå˜åŒ–åºåˆ—
        print(f"   [2/2] ç”Ÿæˆ {frames} å¸§å˜åŒ–åºåˆ—...")
        
        for i in range(frames):
            if progress_callback:
                progress = 0.5 + (i / frames) * 0.4
                progress_callback(int(progress * 100))
            
            # è®¡ç®—å˜åŒ–ç¨‹åº¦
            variation = i / max(frames - 1, 1) if frames > 1 else 0
            
            # åˆ›å»ºè½»å¾®å˜åŒ–çš„å›¾åƒ
            if i == 0:
                # ç¬¬ä¸€å¸§ä½¿ç”¨åŸå§‹å›¾åƒ
                frame_array = base_array.copy()
            else:
                # åç»­å¸§æ·»åŠ è½»å¾®çš„äº®åº¦/å¯¹æ¯”åº¦å˜åŒ–
                frame_array = base_array.copy().astype(np.float32)
                
                # æ·»åŠ è½»å¾®çš„äº®åº¦å˜åŒ–ï¼ˆæ¨¡æ‹Ÿå…‰çº¿å˜åŒ–ï¼‰
                brightness_change = np.sin(variation * np.pi) * 0.05  # -0.05 åˆ° 0.05
                frame_array = frame_array * (1.0 + brightness_change)
                
                # æ·»åŠ è½»å¾®çš„è‰²è°ƒå˜åŒ–
                hue_shift = np.sin(variation * np.pi * 2) * 3  # -3 åˆ° 3
                frame_array[:, :, 0] = np.clip(frame_array[:, :, 0] + hue_shift, 0, 255)
                
                # æ·»åŠ è½»å¾®çš„ç¼©æ”¾æ•ˆæœï¼ˆæ¨¡æ‹Ÿè¿åŠ¨ï¼‰
                zoom_factor = 1.0 + variation * 0.02  # 1.0 åˆ° 1.02
                h, w = frame_array.shape[:2]
                new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)
                
                from PIL import Image
                temp_img = Image.fromarray(frame_array.astype(np.uint8))
                temp_img = temp_img.resize((new_w, new_h), Image.Resampling.LANCZOS)
                
                # è£å‰ªå›åŸå§‹å°ºå¯¸
                left = (new_w - w) // 2
                top = (new_h - h) // 2
                temp_img = temp_img.crop((left, top, left + w, top + h))
                
                frame_array = np.array(temp_img)
            
            # ç¡®ä¿æ•°å€¼èŒƒå›´æ­£ç¡®
            frame_array = np.clip(frame_array, 0, 255).astype(np.uint8)
            video_frames.append(frame_array)
            
            if (i + 1) % 4 == 0 or i == frames - 1:
                print(f"   ç”Ÿæˆè¿›åº¦: {i+1}/{frames} å¸§")
        
        print(f"   âœ“ ç”Ÿæˆäº† {len(video_frames)} å¸§")
        
        # å°†å¸§åºåˆ—è½¬æ¢ä¸ºè§†é¢‘
        print("   ç¼–ç è§†é¢‘...")
        import cv2
        
        temp_video_path = f"temp_video_{seed}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
        
        for frame in video_frames:
            # è½¬æ¢ RGB åˆ° BGRï¼ˆOpenCV æ ¼å¼ï¼‰
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            out.write(frame_bgr)
        
        out.release()
        
        # è¯»å–è§†é¢‘æ–‡ä»¶å¹¶è½¬æ¢ä¸º base64
        with open(temp_video_path, 'rb') as f:
            video_bytes = f.read()
            video_base64 = base64.b64encode(video_bytes).decode('utf-8')
            video_data_url = f"data:video/mp4;base64,{video_base64}"
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(temp_video_path)
        except:
            pass
        
        print("âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ")
        
        return {
            'success': True,
            'video': video_data_url,
            'seed': seed,
            'prompt': prompt,
            'width': width,
            'height': height,
            'frames': frames,
            'fps': fps,
            'model_name': model_info['name'],
            'note': f'âœ… ä½¿ç”¨ Stable Diffusion ç”Ÿæˆ {frames} å¸§è§†é¢‘'
        }
    
    def _generate_with_wanvideo(self, params, model_info, seed, progress_callback=None):
        """ä½¿ç”¨ WanVideo å·¥ä½œæµç”Ÿæˆè§†é¢‘"""
        import random
        import numpy as np
        
        prompt = params.get('prompt', '')
        negative_prompt = params.get('negative_prompt', '')
        width = params.get('width', 512)
        height = params.get('height', 512)
        frames = params.get('frames', 16)
        steps = params.get('steps', 20)
        cfg_scale = params.get('cfg_scale', 7.5)
        fps = params.get('fps', 8)
        
        print(f"   æç¤ºè¯: {prompt[:50]}...")
        print(f"   å‚æ•°: {frames}å¸§, {steps}æ­¥, CFG={cfg_scale}")
        
        # ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ¨¡å‹åç§°
        model_path = model_info['path']
        model_name = os.path.basename(model_path)
        
        try:
            # è°ƒç”¨ WanVideo å·¥ä½œæµ
            video_array, metadata = self.wan_video_workflow.generate_video(
                positive_prompt=prompt,
                negative_prompt=negative_prompt,
                model_name=model_name,
                vae_name="Wan2_1_VAE_bf16.safetensors",
                t5_model="google/t5-v1_1-xxl",
                width=width,
                height=height,
                num_frames=frames,
                steps=steps,
                cfg=cfg_scale,
                shift=1.0,
                seed=seed,
                scheduler="unipc",
                denoise_strength=1.0,
                quantization="fp8_e4m3fn_fast",
                attention_mode="auto",
                lora_enabled=False,
                lora_name="",
                lora_strength=1.0,
                compile_enabled=False,
                compile_backend="inductor",
                block_swap_enabled=False,
                blocks_to_swap=0,
                output_format="mp4",
                fps=fps,
                progress_callback=progress_callback
            )
            
            print(f"   âœ“ ç”Ÿæˆäº† {len(video_array)} å¸§")
            
            # è½¬æ¢ä¸ºè§†é¢‘
            print("   ç¼–ç è§†é¢‘...")
            import cv2
            
            temp_video_path = f"temp_video_{seed}.mp4"
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
            
            for frame in video_array:
                # è½¬æ¢ RGB åˆ° BGR
                if frame.shape[2] == 3:
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                else:
                    frame_bgr = frame
                out.write(frame_bgr)
            
            out.release()
            
            # è¯»å–å¹¶è½¬æ¢ä¸º base64
            with open(temp_video_path, 'rb') as f:
                video_bytes = f.read()
                video_base64 = base64.b64encode(video_bytes).decode('utf-8')
                video_data_url = f"data:video/mp4;base64,{video_base64}"
            
            try:
                os.remove(temp_video_path)
            except:
                pass
            
            print("âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ")
            
            return {
                'success': True,
                'video': video_data_url,
                'seed': seed,
                'prompt': prompt,
                'width': width,
                'height': height,
                'frames': frames,
                'fps': fps,
                'model_name': model_info['name'],
                'note': f'âœ… ä½¿ç”¨ WanVideo å·¥ä½œæµå’Œ {model_info["name"]} ç”Ÿæˆ'
            }
        except Exception as e:
            print(f"âŒ WanVideo å·¥ä½œæµç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _generate_with_video_model(self, params, model_info, seed, progress_callback=None):
        """ä½¿ç”¨è§†é¢‘æ¨¡å‹ç›´æ¥ç”Ÿæˆè§†é¢‘"""
        import random
        import numpy as np
        
        prompt = params.get('prompt', '')
        negative_prompt = params.get('negative_prompt', '')
        width = params.get('width', 512)
        height = params.get('height', 512)
        frames = params.get('frames', 16)
        steps = params.get('steps', 20)
        cfg_scale = params.get('cfg_scale', 7.5)
        fps = params.get('fps', 8)
        
        print(f"   æç¤ºè¯: {prompt[:50]}...")
        print(f"   å‚æ•°: {frames}å¸§, {steps}æ­¥, CFG={cfg_scale}")
        
        # å…ˆç”Ÿæˆåˆå§‹å›¾åƒï¼ˆè§†é¢‘æ¨¡å‹é€šå¸¸éœ€è¦åˆå§‹å›¾åƒï¼‰
        if not self.image_model_loaded:
            self.load_image_model()
        
        generator = torch.Generator(device=self.device).manual_seed(seed)
        
        print("   [1/2] ç”Ÿæˆåˆå§‹å›¾åƒ...")
        with torch.no_grad():
            init_image = self.image_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=cfg_scale,
                generator=generator
            ).images[0]
        
        print("   [2/2] ä½¿ç”¨è§†é¢‘æ¨¡å‹ç”Ÿæˆè§†é¢‘...")
        
        # ä½¿ç”¨è§†é¢‘æ¨¡å‹ç”Ÿæˆ
        generator = torch.Generator(device=self.device).manual_seed(seed)
        
        with torch.no_grad():
            video_frames = self.video_pipe(
                init_image,
                decode_chunk_size=8,
                generator=generator,
                num_frames=frames
            ).frames[0]
        
        print(f"   âœ“ ç”Ÿæˆäº† {len(video_frames)} å¸§")
        
        # è½¬æ¢ä¸ºè§†é¢‘
        print("   ç¼–ç è§†é¢‘...")
        import cv2
        
        temp_video_path = f"temp_video_{seed}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
        
        for frame in video_frames:
            # ç¡®ä¿æ˜¯ numpy æ•°ç»„
            if hasattr(frame, 'numpy'):
                frame = frame.numpy()
            frame = np.array(frame)
            
            # è½¬æ¢ RGB åˆ° BGR
            if frame.shape[2] == 3:
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            else:
                frame_bgr = frame
            
            out.write(frame_bgr)
        
        out.release()
        
        # è¯»å–å¹¶è½¬æ¢ä¸º base64
        with open(temp_video_path, 'rb') as f:
            video_bytes = f.read()
            video_base64 = base64.b64encode(video_bytes).decode('utf-8')
            video_data_url = f"data:video/mp4;base64,{video_base64}"
        
        try:
            os.remove(temp_video_path)
        except:
            pass
        
        print("âœ… è§†é¢‘ç”ŸæˆæˆåŠŸ")
        
        return {
            'success': True,
            'video': video_data_url,
            'seed': seed,
            'prompt': prompt,
            'width': width,
            'height': height,
            'frames': frames,
            'fps': fps,
            'model_name': model_info['name'],
            'note': f'âœ… ä½¿ç”¨ {model_info["name"]} è§†é¢‘æ¨¡å‹ç”Ÿæˆ'
        }
    
    def _generate_mock_video(self, params, model_info):
        """ç”Ÿæˆæ¨¡æ‹Ÿè§†é¢‘"""
        import random
        time.sleep(3)
        
        # è¿”å›ä¸€ä¸ªç®€å•çš„è§†é¢‘ URLï¼ˆå®é™…åº”è¯¥æ˜¯ base64 ç¼–ç çš„ MP4ï¼‰
        mock_video = "data:video/mp4;base64,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"
        
        return {
            'success': True,
            'video': mock_video,
            'seed': params.get('seed') or random.randint(0, 2147483647),
            'prompt': params.get('prompt', ''),
            'width': params.get('width', 512),
            'height': params.get('height', 512),
            'frames': params.get('frames', 16),
            'fps': params.get('fps', 8),
            'model_name': model_info['name'],
            'note': f'âš ï¸ è§†é¢‘ç”ŸæˆåŠŸèƒ½å¼€å‘ä¸­\nå½“å‰æ¨¡å‹: {model_info["name"]}\næ¨¡å‹è·¯å¾„: {model_info["path"]}\néœ€è¦å®ç° I2V æ¨¡å‹åŠ è½½'
        }

# åˆ›å»ºå…¨å±€ç”Ÿæˆå™¨
generator = MultiModalGenerator()

# å·¥ä½œçº¿ç¨‹
worker_running = False
worker_thread = None

def worker_loop():
    """å¤„ç†ä»»åŠ¡é˜Ÿåˆ—"""
    global worker_running
    print("ğŸ”„ Worker thread started")
    
    while worker_running:
        try:
            task_id = task_queue.get(timeout=1.0)
            task = tasks.get(task_id)
            
            if not task:
                continue
            
            task.status = TaskStatus.RUNNING
            task.started_at = datetime.now()
            task.progress = 5
            
            print(f"ğŸ“ Processing task: {task_id}")
            print(f"   ä»»åŠ¡ç±»å‹: {task.task_type}")
            print(f"   æç¤ºè¯: {task.params.get('prompt', 'N/A')[:50]}...")
            
            try:
                def progress_callback(progress):
                    task.progress = progress
                
                # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä¸åŒçš„ç”Ÿæˆ
                if task.task_type == 'generate' or task.task_type == 'text_to_image':
                    print("   â†’ æ‰§è¡Œæ–‡ç”Ÿå›¾")
                    result = generator.generate_image(task.params, progress_callback)
                elif task.task_type == 'text_to_video':
                    print("   â†’ æ‰§è¡Œæ–‡ç”Ÿè§†é¢‘")
                    result = generator.generate_video(task.params, progress_callback)
                else:
                    raise ValueError(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {task.task_type}")
                
                task.status = TaskStatus.COMPLETED
                task.result = result
                task.progress = 100
                task.completed_at = datetime.now()
                
                print(f"âœ… Task completed: {task_id}")
                
            except Exception as e:
                task.status = TaskStatus.FAILED
                task.error = str(e)
                task.completed_at = datetime.now()
                print(f"âŒ Task failed: {task_id} - {e}")
                
        except Empty:
            continue
        except Exception as e:
            print(f"âŒ Worker error: {e}")
    
    print("ğŸ›‘ Worker thread stopped")

# API è·¯ç”±
@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'torch_available': TORCH_AVAILABLE,
        'image_model_loaded': generator.image_model_loaded,
        'video_model_loaded': generator.video_model_loaded,
        'device': generator.device if TORCH_AVAILABLE else 'N/A',
        'tasks_pending': task_queue.qsize(),
        'tasks_total': len(tasks),
        'model_paths': MODEL_PATHS
    })

@app.route('/api/session/create', methods=['POST'])
def create_session():
    """åˆ›å»ºä¼šè¯"""
    data = request.get_json() or {}
    session_id = str(uuid.uuid4())
    
    return jsonify({
        'success': True,
        'session_id': session_id,
        'session': {
            'session_id': session_id,
            'client_type': data.get('client_type', 'web'),
            'created_at': datetime.now().isoformat()
        }
    })

@app.route('/api/task/submit', methods=['POST'])
def submit_task():
    """æäº¤ä»»åŠ¡"""
    try:
        data = request.get_json() or {}
        
        task_type = data.get('task_type', 'generate')
        params = data.get('params', {})
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = Task(task_id, task_type, params)
        
        # ä¿å­˜ä»»åŠ¡
        tasks[task_id] = task
        task_queue.put(task_id)
        
        print(f"ğŸ“¥ Task submitted: {task_id} ({task_type})")
        print(f"   Prompt: {params.get('prompt', 'N/A')[:50]}...")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'task': task.to_dict()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 400

@app.route('/api/task/<task_id>', methods=['GET'])
def get_task(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    task = tasks.get(task_id)
    
    if not task:
        return jsonify({
            'success': False,
            'error': 'Task not found'
        }), 404
    
    return jsonify({
        'success': True,
        'task': task.to_dict()
    })

@app.route('/api/task/<task_id>/cancel', methods=['POST'])
def cancel_task(task_id):
    """å–æ¶ˆä»»åŠ¡"""
    task = tasks.get(task_id)
    
    if not task:
        return jsonify({
            'success': False,
            'error': 'Task not found'
        }), 404
    
    if task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
        return jsonify({
            'success': False,
            'error': 'Task already finished'
        }), 400
    
    task.status = TaskStatus.FAILED
    task.error = 'Cancelled by user'
    task.completed_at = datetime.now()
    
    return jsonify({
        'success': True,
        'task': task.to_dict()
    })

@app.route('/api/tasks', methods=['GET'])
def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
    return jsonify({
        'success': True,
        'tasks': [task.to_dict() for task in tasks.values()],
        'count': len(tasks)
    })

@app.route('/api/models', methods=['GET'])
def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return jsonify({
        'success': True,
        'models': {
            'image_models': [
                {
                    'id': key,
                    'name': value['name'],
                    'path': value['path'],
                    'type': value['type']
                }
                for key, value in IMAGE_MODELS.items()
            ],
            'video_models': [
                {
                    'id': key,
                    'name': value['name'],
                    'path': value['path'],
                    'type': value['type'],
                    'description': value.get('description', '')
                }
                for key, value in VIDEO_MODELS.items()
            ],
            'default_image_model': DEFAULT_IMAGE_MODEL,
            'default_video_model': DEFAULT_VIDEO_MODEL
        }
    })

@app.route('/api/device', methods=['GET'])
def device_info():
    """è·å–è®¾å¤‡ä¿¡æ¯"""
    if not TORCH_AVAILABLE:
        return jsonify({
            'success': True,
            'device': {
                'device': 'N/A',
                'note': 'PyTorch not installed'
            }
        })
    
    info = {'device': generator.device}
    
    if generator.device == 'cuda':
        info['device_name'] = torch.cuda.get_device_name(0)
        props = torch.cuda.get_device_properties(0)
        info['memory_total'] = props.total_memory
        info['memory_allocated'] = torch.cuda.memory_allocated(0)
    
    return jsonify({
        'success': True,
        'device': info
    })

@app.route('/')
def index():
    """API é¦–é¡µ"""
    return jsonify({
        'name': 'Genesis API Server (Multi-Modal)',
        'version': '1.1.0',
        'status': 'running',
        'torch_available': TORCH_AVAILABLE,
        'image_model_loaded': generator.image_model_loaded,
        'video_model_loaded': generator.video_model_loaded,
        'model_paths': MODEL_PATHS,
        'endpoints': {
            'GET  /health': 'Health check',
            'POST /api/session/create': 'Create session',
            'POST /api/task/submit': 'Submit task (text_to_image or text_to_video)',
            'GET  /api/task/<id>': 'Get task status',
            'POST /api/task/<id>/cancel': 'Cancel task',
            'GET  /api/tasks': 'List all tasks',
            'GET  /api/models': 'List models',
            'GET  /api/device': 'Device info'
        }
    })

def start_worker():
    """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
    global worker_running, worker_thread
    
    worker_running = True
    worker_thread = threading.Thread(target=worker_loop, daemon=True)
    worker_thread.start()

def stop_worker():
    """åœæ­¢å·¥ä½œçº¿ç¨‹"""
    global worker_running
    
    worker_running = False
    if worker_thread:
        worker_thread.join(timeout=5.0)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("Genesis API Server - Multi-Modal (Image + Video)")
    print("=" * 70)
    print()
    
    print("ğŸ“ å¯ç”¨æ¨¡å‹:")
    print()
    print("  å›¾åƒæ¨¡å‹:")
    for key, value in IMAGE_MODELS.items():
        print(f"    [{key}] {value['name']}")
        print(f"        {value['path']}")
    print()
    print("  è§†é¢‘æ¨¡å‹:")
    for key, value in VIDEO_MODELS.items():
        print(f"    [{key}] {value['name']}")
        print(f"        {value['path']}")
        print(f"        {value.get('description', '')}")
    print()
    print(f"  é»˜è®¤å›¾åƒæ¨¡å‹: {IMAGE_MODELS[DEFAULT_IMAGE_MODEL]['name']}")
    print(f"  é»˜è®¤è§†é¢‘æ¨¡å‹: {VIDEO_MODELS[DEFAULT_VIDEO_MODEL]['name']}")
    print()
    
    if TORCH_AVAILABLE:
        print("âœ… PyTorch å¯ç”¨")
        print(f"   è®¾å¤‡: {generator.device}")
        if generator.device == "cuda":
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    
    print()
    print("ğŸš€ Starting server...")
    print()
    print("ğŸ“¡ Server URL: http://localhost:5000")
    print("ğŸ” Health check: http://localhost:5000/health")
    print("ğŸ“š API docs: http://localhost:5000/")
    print()
    print("=" * 70)
    print()
    
    # å¯åŠ¨å·¥ä½œçº¿ç¨‹
    start_worker()
    
    try:
        # å¯åŠ¨ Flask æœåŠ¡å™¨
        app.run(
            host='0.0.0.0',
            port=5000,
            debug=False,
            use_reloader=False
        )
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Shutting down...")
    finally:
        stop_worker()
        print("âœ… Server stopped")

if __name__ == "__main__":
    main()
