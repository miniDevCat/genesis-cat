#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Genesis API Server - çœŸå®å›¾åƒç”Ÿæˆç‰ˆæœ¬
ä½¿ç”¨ Stable Diffusion ç”ŸæˆçœŸå®å›¾åƒ
"""

import sys
import os
from pathlib import Path
import base64
from io import BytesIO

# ç¦ç”¨æ‰€æœ‰å¯é€‰çš„æ³¨æ„åŠ›ä¼˜åŒ–åº“ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜
os.environ['DIFFUSERS_DISABLE_FLASH_ATTENTION'] = '1'
os.environ['DISABLE_SAGE_ATTENTION'] = '1'
os.environ['ATTN_BACKEND'] = 'pytorch'  # å¼ºåˆ¶ä½¿ç”¨ PyTorch åŸç”Ÿå®ç°

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from flask import Flask, request, jsonify
    from flask_cors import CORS
    FLASK_AVAILABLE = True
except ImportError:
    print("=" * 70)
    print("é”™è¯¯: Flask æœªå®‰è£…")
    print("=" * 70)
    print()
    print("è¯·å®‰è£…å¿…è¦çš„ä¾èµ–:")
    print("  pip install flask flask-cors")
    print()
    sys.exit(1)

try:
    import torch
    from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler
    DIFFUSERS_AVAILABLE = True
    print("âœ“ Diffusers å·²å®‰è£…")
except ImportError as e:
    print("=" * 70)
    print("è­¦å‘Š: Diffusers æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    print("=" * 70)
    print()
    print(f"é”™è¯¯ä¿¡æ¯: {e}")
    print()
    print("å¦‚éœ€çœŸå®ç”Ÿæˆå›¾åƒï¼Œè¯·å®‰è£…:")
    print("  pip install torch torchvision")
    print("  pip install diffusers transformers accelerate")
    print()
    print("å¦‚æœé‡åˆ° flash_attn é”™è¯¯ï¼Œè¯·å¸è½½å®ƒ:")
    print("  pip uninstall flash-attn -y")
    print()
    DIFFUSERS_AVAILABLE = False

import uuid
import time
import threading
from queue import Queue, Empty
from datetime import datetime

# åˆ›å»º Flask åº”ç”¨
app = Flask(__name__)
CORS(app)

# ä»»åŠ¡å­˜å‚¨
tasks = {}
task_queue = Queue()

# ä»»åŠ¡çŠ¶æ€å¸¸é‡
class TaskStatus:
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

# ä»»åŠ¡ç±»
class Task:
    def __init__(self, task_id, task_type, params):
        self.task_id = task_id
        self.task_type = task_type
        self.params = params
        self.status = TaskStatus.PENDING
        self.progress = 0
        self.result = None
        self.error = None
        self.created_at = datetime.now()
        self.started_at = None
        self.completed_at = None
    
    def to_dict(self):
        return {
            'task_id': self.task_id,
            'task_type': self.task_type,
            'params': self.params,
            'status': self.status,
            'progress': self.progress,
            'result': self.result,
            'error': self.error,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'started_at': self.started_at.isoformat() if self.started_at else None,
            'completed_at': self.completed_at.isoformat() if self.completed_at else None
        }

# å…¨å±€ç”Ÿæˆå™¨
class ImageGenerator:
    def __init__(self):
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu" if DIFFUSERS_AVAILABLE else "cpu"
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32 if DIFFUSERS_AVAILABLE else None
        self.model_loaded = False
        
    def load_model(self, model_id="runwayml/stable-diffusion-v1-5"):
        """åŠ è½½æ¨¡å‹"""
        if not DIFFUSERS_AVAILABLE:
            print("âš ï¸  Diffusers æœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
            return False
            
        if self.model_loaded:
            print("âœ“ æ¨¡å‹å·²åŠ è½½")
            return True
            
        try:
            print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_id}")
            print(f"   è®¾å¤‡: {self.device}")
            
            self.pipe = StableDiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=self.dtype,
                safety_checker=None,
                requires_safety_checker=False
            )
            
            self.pipe = self.pipe.to(self.device)
            
            # ä½¿ç”¨ Euler è°ƒåº¦å™¨
            self.pipe.scheduler = EulerDiscreteScheduler.from_config(
                self.pipe.scheduler.config
            )
            
            # å¯ç”¨ä¼˜åŒ–
            if self.device == "cuda":
                self.pipe.enable_attention_slicing()
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                    print("âœ“ å¯ç”¨ xformers ä¼˜åŒ–")
                except:
                    print("â„¹ xformers æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›")
            
            self.model_loaded = True
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate(self, params, progress_callback=None):
        """ç”Ÿæˆå›¾åƒ"""
        if not DIFFUSERS_AVAILABLE:
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return self._generate_mock(params)
        
        if not self.model_loaded:
            if not self.load_model():
                raise Exception("æ¨¡å‹åŠ è½½å¤±è´¥")
        
        try:
            prompt = params.get('prompt', '')
            negative_prompt = params.get('negative_prompt', '')
            width = params.get('width', 512)
            height = params.get('height', 512)
            steps = params.get('steps', 20)
            cfg_scale = params.get('cfg_scale', 7.0)
            seed = params.get('seed')
            
            # è®¾ç½®éšæœºç§å­
            if seed is None or seed == -1:
                seed = torch.randint(0, 2**32 - 1, (1,)).item()
            
            generator = torch.Generator(device=self.device).manual_seed(int(seed))
            
            print(f"ğŸ¨ å¼€å§‹ç”Ÿæˆå›¾åƒ...")
            print(f"   æç¤ºè¯: {prompt[:50]}...")
            print(f"   å°ºå¯¸: {width}x{height}")
            print(f"   æ­¥æ•°: {steps}")
            
            # ç”Ÿæˆå›¾åƒ
            def callback(step, timestep, latents):
                if progress_callback:
                    progress = int((step / steps) * 80) + 10  # 10-90%
                    progress_callback(progress)
            
            result = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=cfg_scale,
                generator=generator,
                callback=callback,
                callback_steps=1
            )
            
            image = result.images[0]
            
            # è½¬æ¢ä¸º base64
            buffered = BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            img_data_url = f"data:image/png;base64,{img_str}"
            
            print("âœ… å›¾åƒç”ŸæˆæˆåŠŸ")
            
            return {
                'success': True,
                'image': img_data_url,
                'seed': seed,
                'prompt': prompt,
                'width': width,
                'height': height,
            }
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _generate_mock(self, params):
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®"""
        import random
        time.sleep(2)  # æ¨¡æ‹Ÿç”Ÿæˆæ—¶é—´
        
        # 1x1 é€æ˜ PNG
        mock_image = "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=="
        
        return {
            'success': True,
            'image': mock_image,
            'seed': params.get('seed') or random.randint(0, 2147483647),
            'prompt': params.get('prompt', ''),
            'width': params.get('width', 512),
            'height': params.get('height', 512),
            'note': 'âš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œè¯·å®‰è£… diffusers ä»¥ç”ŸæˆçœŸå®å›¾åƒ'
        }

# åˆ›å»ºå…¨å±€ç”Ÿæˆå™¨
generator = ImageGenerator()

# å·¥ä½œçº¿ç¨‹
worker_running = False
worker_thread = None

def worker_loop():
    """å¤„ç†ä»»åŠ¡é˜Ÿåˆ—"""
    global worker_running
    print("ğŸ”„ Worker thread started")
    
    while worker_running:
        try:
            task_id = task_queue.get(timeout=1.0)
            task = tasks.get(task_id)
            
            if not task:
                continue
            
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            task.status = TaskStatus.RUNNING
            task.started_at = datetime.now()
            task.progress = 5
            
            print(f"ğŸ“ Processing task: {task_id}")
            
            try:
                # è¿›åº¦å›è°ƒ
                def progress_callback(progress):
                    task.progress = progress
                
                # æ‰§è¡Œç”Ÿæˆ
                result = generator.generate(task.params, progress_callback)
                
                # å®Œæˆ
                task.status = TaskStatus.COMPLETED
                task.result = result
                task.progress = 100
                task.completed_at = datetime.now()
                
                print(f"âœ… Task completed: {task_id}")
                
            except Exception as e:
                task.status = TaskStatus.FAILED
                task.error = str(e)
                task.completed_at = datetime.now()
                print(f"âŒ Task failed: {task_id} - {e}")
                
        except Empty:
            continue
        except Exception as e:
            print(f"âŒ Worker error: {e}")
    
    print("ğŸ›‘ Worker thread stopped")

# API è·¯ç”±
@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'diffusers_available': DIFFUSERS_AVAILABLE,
        'model_loaded': generator.model_loaded if DIFFUSERS_AVAILABLE else False,
        'device': generator.device if DIFFUSERS_AVAILABLE else 'N/A',
        'tasks_pending': task_queue.qsize(),
        'tasks_total': len(tasks)
    })

@app.route('/api/session/create', methods=['POST'])
def create_session():
    """åˆ›å»ºä¼šè¯"""
    data = request.get_json() or {}
    session_id = str(uuid.uuid4())
    
    return jsonify({
        'success': True,
        'session_id': session_id,
        'session': {
            'session_id': session_id,
            'client_type': data.get('client_type', 'web'),
            'created_at': datetime.now().isoformat()
        }
    })

@app.route('/api/task/submit', methods=['POST'])
def submit_task():
    """æäº¤ä»»åŠ¡"""
    try:
        data = request.get_json() or {}
        
        task_type = data.get('task_type', 'generate')
        params = data.get('params', {})
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = Task(task_id, task_type, params)
        
        # ä¿å­˜ä»»åŠ¡
        tasks[task_id] = task
        task_queue.put(task_id)
        
        print(f"ğŸ“¥ Task submitted: {task_id}")
        print(f"   Prompt: {params.get('prompt', 'N/A')[:50]}...")
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'task': task.to_dict()
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 400

@app.route('/api/task/<task_id>', methods=['GET'])
def get_task(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    task = tasks.get(task_id)
    
    if not task:
        return jsonify({
            'success': False,
            'error': 'Task not found'
        }), 404
    
    return jsonify({
        'success': True,
        'task': task.to_dict()
    })

@app.route('/api/task/<task_id>/cancel', methods=['POST'])
def cancel_task(task_id):
    """å–æ¶ˆä»»åŠ¡"""
    task = tasks.get(task_id)
    
    if not task:
        return jsonify({
            'success': False,
            'error': 'Task not found'
        }), 404
    
    if task.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
        return jsonify({
            'success': False,
            'error': 'Task already finished'
        }), 400
    
    task.status = TaskStatus.FAILED
    task.error = 'Cancelled by user'
    task.completed_at = datetime.now()
    
    return jsonify({
        'success': True,
        'task': task.to_dict()
    })

@app.route('/api/tasks', methods=['GET'])
def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
    return jsonify({
        'success': True,
        'tasks': [task.to_dict() for task in tasks.values()],
        'count': len(tasks)
    })

@app.route('/api/models', methods=['GET'])
def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return jsonify({
        'success': True,
        'models': {
            'checkpoints': ['runwayml/stable-diffusion-v1-5', 'stabilityai/stable-diffusion-2-1'],
            'loras': [],
            'vae': []
        }
    })

@app.route('/api/device', methods=['GET'])
def device_info():
    """è·å–è®¾å¤‡ä¿¡æ¯"""
    if not DIFFUSERS_AVAILABLE:
        return jsonify({
            'success': True,
            'device': {
                'device': 'N/A',
                'note': 'Diffusers not installed'
            }
        })
    
    info = {'device': generator.device}
    
    if generator.device == 'cuda':
        info['device_name'] = torch.cuda.get_device_name(0)
        props = torch.cuda.get_device_properties(0)
        info['memory_total'] = props.total_memory
        info['memory_allocated'] = torch.cuda.memory_allocated(0)
    
    return jsonify({
        'success': True,
        'device': info
    })

@app.route('/')
def index():
    """API é¦–é¡µ"""
    return jsonify({
        'name': 'Genesis API Server (Real Generation)',
        'version': '1.0.0',
        'status': 'running',
        'diffusers_available': DIFFUSERS_AVAILABLE,
        'model_loaded': generator.model_loaded if DIFFUSERS_AVAILABLE else False,
        'endpoints': {
            'GET  /health': 'Health check',
            'POST /api/session/create': 'Create session',
            'POST /api/task/submit': 'Submit task',
            'GET  /api/task/<id>': 'Get task status',
            'POST /api/task/<id>/cancel': 'Cancel task',
            'GET  /api/tasks': 'List all tasks',
            'GET  /api/models': 'List models',
            'GET  /api/device': 'Device info'
        }
    })

def start_worker():
    """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
    global worker_running, worker_thread
    
    worker_running = True
    worker_thread = threading.Thread(target=worker_loop, daemon=True)
    worker_thread.start()

def stop_worker():
    """åœæ­¢å·¥ä½œçº¿ç¨‹"""
    global worker_running
    
    worker_running = False
    if worker_thread:
        worker_thread.join(timeout=5.0)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("Genesis API Server - Real Image Generation")
    print("=" * 70)
    print()
    
    if DIFFUSERS_AVAILABLE:
        print("âœ… Diffusers å¯ç”¨")
        print(f"   è®¾å¤‡: {generator.device}")
        if generator.device == "cuda":
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print()
        print("ğŸ“¥ é¦–æ¬¡ç”Ÿæˆæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 4GBï¼‰")
        print("   æ¨¡å‹ä¼šç¼“å­˜åˆ°: ~/.cache/huggingface/")
    else:
        print("âš ï¸  Diffusers æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        print()
        print("å¦‚éœ€çœŸå®ç”Ÿæˆï¼Œè¯·å®‰è£…:")
        print("  pip install torch torchvision")
        print("  pip install diffusers transformers accelerate")
    
    print()
    print("ğŸš€ Starting server...")
    print()
    print("ğŸ“¡ Server URL: http://localhost:5000")
    print("ğŸ” Health check: http://localhost:5000/health")
    print("ğŸ“š API docs: http://localhost:5000/")
    print()
    print("=" * 70)
    print()
    
    # å¯åŠ¨å·¥ä½œçº¿ç¨‹
    start_worker()
    
    try:
        # å¯åŠ¨ Flask æœåŠ¡å™¨
        app.run(
            host='0.0.0.0',
            port=5000,
            debug=False,
            use_reloader=False
        )
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Shutting down...")
    finally:
        stop_worker()
        print("âœ… Server stopped")

if __name__ == "__main__":
    main()
