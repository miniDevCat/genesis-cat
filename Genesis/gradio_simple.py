#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Genesis Gradio ç®€åŒ–ç‰ˆ - é¿å… Gradio 5.x çš„å¯åŠ¨é—®é¢˜
"""

import sys
from pathlib import Path
import torch
import os

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…ä¸€äº›é—®é¢˜
os.environ['GRADIO_SERVER_NAME'] = '127.0.0.1'
os.environ['GRADIO_SERVER_PORT'] = '7860'

sys.path.insert(0, str(Path(__file__).parent))

import gradio as gr
from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler

# å¯¼å…¥ folder_paths
import importlib.util
spec = importlib.util.spec_from_file_location(
    "folder_paths", 
    Path(__file__).parent / "core" / "folder_paths.py"
)
folder_paths = importlib.util.module_from_spec(spec)
spec.loader.exec_module(folder_paths)


class SimpleGenerator:
    def __init__(self):
        self.pipe = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        
    def load_model(self, model_name):
        """åŠ è½½æ¨¡å‹"""
        try:
            print(f"\n{'='*60}")
            print(f"åŠ è½½æ¨¡å‹: {model_name}")
            print(f"{'='*60}")
            
            if model_name.startswith("HF:"):
                model_path = model_name[3:]
                print(f"HuggingFace æ¨¡å‹: {model_path}")
            else:
                model_path = folder_paths.get_full_path('checkpoints', model_name)
                print(f"æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
            
            print(f"è®¾å¤‡: {self.device}")
            print("æ­£åœ¨åŠ è½½...")
            
            # åŠ è½½æ¨¡å‹
            try:
                self.pipe = StableDiffusionPipeline.from_single_file(
                    model_path,
                    torch_dtype=self.dtype,
                    safety_checker=None,
                    requires_safety_checker=False
                )
            except:
                self.pipe = StableDiffusionPipeline.from_pretrained(
                    model_path,
                    torch_dtype=self.dtype,
                    safety_checker=None,
                    requires_safety_checker=False
                )
            
            self.pipe = self.pipe.to(self.device)
            self.pipe.scheduler = EulerDiscreteScheduler.from_config(
                self.pipe.scheduler.config
            )
            
            if self.device == "cuda":
                self.pipe.enable_attention_slicing()
                try:
                    self.pipe.enable_xformers_memory_efficient_attention()
                except:
                    pass
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}"
        except Exception as e:
            error = f"âŒ åŠ è½½å¤±è´¥: {str(e)}"
            print(error)
            return error
    
    def generate(self, prompt, negative_prompt, width, height, steps, cfg, seed):
        """ç”Ÿæˆå›¾åƒ"""
        if self.pipe is None:
            return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        try:
            if seed == -1:
                seed = torch.randint(0, 2**32-1, (1,)).item()
            
            generator = torch.Generator(device=self.device).manual_seed(int(seed))
            
            print(f"\nç”Ÿæˆä¸­: {prompt[:50]}...")
            
            result = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_inference_steps=steps,
                guidance_scale=cfg,
                generator=generator
            )
            
            image = result.images[0]
            info = f"âœ… ç”Ÿæˆå®Œæˆ!\n\n**æç¤ºè¯:** {prompt}\n\n**å‚æ•°:** {width}x{height}, {steps}æ­¥, CFG {cfg}, ç§å­ {seed}"
            
            return image, info
        except Exception as e:
            return None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


# å…¨å±€ç”Ÿæˆå™¨
gen = SimpleGenerator()

# è·å–æ¨¡å‹åˆ—è¡¨
models = folder_paths.get_filename_list('checkpoints')
model_choices = ["HF:runwayml/stable-diffusion-v1-5", "HF:stabilityai/stable-diffusion-2-1"]
if models:
    model_choices.extend(models)

# åˆ›å»ºç•Œé¢
with gr.Blocks(title="Genesis AI", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¨ Genesis AI å›¾åƒç”Ÿæˆå™¨")
    
    with gr.Row():
        with gr.Column():
            model_select = gr.Dropdown(
                label="é€‰æ‹©æ¨¡å‹",
                choices=model_choices,
                value=model_choices[0]
            )
            load_btn = gr.Button("ğŸ“¥ åŠ è½½æ¨¡å‹", variant="secondary")
            status = gr.Textbox(label="çŠ¶æ€", value="æœªåŠ è½½", lines=2)
            
            gr.Markdown("---")
            
            prompt = gr.Textbox(
                label="æç¤ºè¯",
                lines=3,
                value="a beautiful landscape, sunset, 4k"
            )
            neg_prompt = gr.Textbox(
                label="è´Ÿå‘æç¤ºè¯",
                lines=2,
                value="ugly, blurry, low quality"
            )
            
            with gr.Row():
                width = gr.Slider(256, 1024, 512, step=64, label="å®½åº¦")
                height = gr.Slider(256, 1024, 512, step=64, label="é«˜åº¦")
            
            with gr.Row():
                steps = gr.Slider(1, 100, 20, step=1, label="æ­¥æ•°")
                cfg = gr.Slider(1, 20, 7, step=0.5, label="CFG")
            
            seed = gr.Number(label="ç§å­ (-1éšæœº)", value=-1)
            
            gen_btn = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary")
        
        with gr.Column():
            output_img = gr.Image(label="ç”Ÿæˆç»“æœ", type="pil")
            output_info = gr.Markdown("ç­‰å¾…ç”Ÿæˆ...")
    
    # äº‹ä»¶
    load_btn.click(gen.load_model, inputs=[model_select], outputs=[status])
    gen_btn.click(
        gen.generate,
        inputs=[prompt, neg_prompt, width, height, steps, cfg, seed],
        outputs=[output_img, output_info]
    )

if __name__ == "__main__":
    print("="*60)
    print("Genesis AI - ç®€åŒ–ç‰ˆç•Œé¢")
    print("="*60)
    print(f"è®¾å¤‡: {gen.device}")
    print(f"å¯ç”¨æ¨¡å‹: {len(models)} ä¸ªæœ¬åœ°æ¨¡å‹")
    print("="*60)
    
    # ä½¿ç”¨æœ€ç®€å•çš„å¯åŠ¨æ–¹å¼
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        inbrowser=True,
        quiet=False
    )
